{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['indices', 'sci']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "import scipy.io\n",
    "from sklearn import svm\n",
    "from skimage import feature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import csv\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "import math\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv(pred, name):\n",
    "    np.savetxt(name, np.dstack((np.arange(1, len(pred)+1), pred))[0],\"%d,%d\",header=\"Id,Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TRANSFORMED DATA\n",
    "spam_data = scipy.io.loadmat(\"spam-dataset/spam_data.mat\")\n",
    "spam_train = spam_data['training_data']\n",
    "spam_labels = spam_data['training_labels'].T\n",
    "spam_test = spam_data['test_data']\n",
    "spam_width = spam_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1): Code for Decision Trees and Random Forests (plus Testing/Validation/Classifying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Node code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, height, random = False, m = 25):\n",
    "        self.height = height\n",
    "        self.random = random\n",
    "        self.m = m\n",
    "        self.root = None\n",
    "        \n",
    "    def impurity(self, left_label_hist, right_label_hist):\n",
    "        # weighted entropy    \n",
    "        left_size = 0\n",
    "        for key, val in left_label_hist.items():\n",
    "            left_size+=val\n",
    "        right_size = 0\n",
    "        for key, val in right_label_hist.items():\n",
    "            right_size+=val\n",
    "        def calc_entropy(index_set):\n",
    "            total = 0\n",
    "            for key, val in index_set.items():\n",
    "                total+=val\n",
    "            if total == 0:\n",
    "                # return big number\n",
    "                return 9999999999\n",
    "            probs = {}\n",
    "            for key, val in index_set.items():\n",
    "                probs[key] = val/total\n",
    "            entropy_sum = 0\n",
    "            for key in index_set:\n",
    "                if probs[key] == 0:\n",
    "                    # Case where all the samples go to one class\n",
    "                    return 0\n",
    "                entropy_sum += -probs[key] * np.log(probs[key])\n",
    "            return entropy_sum\n",
    "        left_entropy = calc_entropy(left_label_hist)\n",
    "        right_entropy = calc_entropy(right_label_hist)\n",
    "        weighted_entropy = (left_size * left_entropy + right_size * right_entropy)/(left_size + right_size)\n",
    "        return weighted_entropy\n",
    "    \n",
    "    def segmenter(self, splits, data, labels):\n",
    "        # for each feature\n",
    "        feature_entropies = []\n",
    "        best_rules = []\n",
    "        rand_features = []\n",
    "        if self.random:\n",
    "            index = 0\n",
    "            feature_map = {}\n",
    "            while len(rand_features) < self.m:\n",
    "                i = np.random.randint(len(data.T))\n",
    "                if not i in rand_features:\n",
    "                    rand_features.append(i)\n",
    "                    feature_map[index] = i\n",
    "                    index+=1\n",
    "        else:\n",
    "            feature_map = {i:i for i in range(len(data.T))}\n",
    "            rand_features = range(len(data.T))\n",
    "        for feature in rand_features:\n",
    "            curr_list = []\n",
    "            counts_dict_0 = {}\n",
    "            counts_dict_1 = {}\n",
    "            for i in splits:\n",
    "                feature_val = data[i][feature]\n",
    "                label = labels[i]\n",
    "                curr_list.append(feature_val)\n",
    "                if label == 0:\n",
    "                    if feature_val in counts_dict_0:\n",
    "                        counts_dict_0[feature_val] += 1\n",
    "                    else:\n",
    "                        counts_dict_0[feature_val] = 1\n",
    "                    if not feature_val in counts_dict_1:\n",
    "                        counts_dict_1[feature_val] = 0\n",
    "                else:\n",
    "                    if feature_val in counts_dict_1:\n",
    "                        counts_dict_1[feature_val] += 1\n",
    "                    else:\n",
    "                        counts_dict_1[feature_val] = 1\n",
    "                    if not feature_val in counts_dict_0:\n",
    "                        counts_dict_0[feature_val] = 0\n",
    "            unique_list = list(set(curr_list))\n",
    "            unique_list.sort()\n",
    "            \n",
    "            left_label_hist = {0:0, 1:0}\n",
    "            right_label_hist = {0:0, 1:0}\n",
    "            left_label_hist[0]+= counts_dict_0[unique_list[0]]\n",
    "            left_label_hist[1]+= counts_dict_1[unique_list[0]]\n",
    "            for i in range(1, len(unique_list)):\n",
    "                right_label_hist[0]+= counts_dict_0[unique_list[i]]\n",
    "                right_label_hist[1]+= counts_dict_1[unique_list[i]]\n",
    "\n",
    "            lowest_entropy = self.impurity(left_label_hist, right_label_hist)\n",
    "            best_rule = unique_list[0]\n",
    "            # Testing each split\n",
    "            for i in range(1, len(unique_list)-1):\n",
    "                left_label_hist[0]+= counts_dict_0[unique_list[i]]\n",
    "                left_label_hist[1]+= counts_dict_1[unique_list[i]]\n",
    "                right_label_hist[0]-= counts_dict_0[unique_list[i]]\n",
    "                right_label_hist[1]-= counts_dict_1[unique_list[i]]\n",
    "                curr_entropy = self.impurity(left_label_hist, right_label_hist)\n",
    "                if curr_entropy < lowest_entropy:\n",
    "                    lowest_entropy = curr_entropy\n",
    "                    \n",
    "                    best_rule = unique_list[i]\n",
    "            feature_entropies.append(lowest_entropy)\n",
    "            best_rules.append(best_rule)\n",
    "            \n",
    "        # MAKE SURE FEATURE MAPPING IS CORRECT\n",
    "        best_index = np.argmin(feature_entropies)\n",
    "        rule = best_rules[best_index]\n",
    "        best_feature = feature_map[best_index]\n",
    "        return (best_feature, rule)\n",
    "    \n",
    "    def train(self, train_data, train_labels):\n",
    "        S = []\n",
    "        for i in range(train_data.shape[0]):\n",
    "            S.append(i)\n",
    "        def grow_tree(S, train_data, train_labels, height):\n",
    "\n",
    "            def get_majority(labels):\n",
    "                count0 = 0\n",
    "                count1 = 0\n",
    "                for i in range(len(new_labels)):\n",
    "                    if new_labels[i][0] == 0:\n",
    "                        count0+=1\n",
    "                    else:\n",
    "                        count1+=1\n",
    "                if count0 > count1:\n",
    "                    return 0\n",
    "                return 1\n",
    "\n",
    "            same = True\n",
    "            val = train_labels[S[0]]\n",
    "            new_labels = train_labels[S]\n",
    "            for i in S:\n",
    "                if val != train_labels[i]:                    \n",
    "                    same = False\n",
    "                    break\n",
    "            if height == 0:\n",
    "                return Node(None, None, None, get_majority(new_labels))\n",
    "            if same:\n",
    "                return Node(None, None, None, int(val[0]))\n",
    "            else:\n",
    "                feature, split = self.segmenter(S, train_data, train_labels)\n",
    "                S_l = []\n",
    "                S_r = []\n",
    "                for i in S:\n",
    "                    if train_data[i][feature] <= split:\n",
    "                        S_l.append(i)\n",
    "                    else:\n",
    "                        S_r.append(i)\n",
    "                \n",
    "                if len(S_l) == 0 or len(S_r) == 0:\n",
    "                    return Node(None, None, None, get_majority(new_labels))\n",
    "                return Node((feature, split), grow_tree(S_l, train_data, train_labels, height-1), grow_tree(S_r, train_data, train_labels, height-1))\n",
    "        self.root = grow_tree(S, train_data, train_labels, self.height)\n",
    "    \n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        predicted_labels = np.zeros((len(test_data), 1))\n",
    "        for i in range(len(test_data)):\n",
    "            curr_node = self.root\n",
    "            while curr_node.label==None:\n",
    "                feature, rule = curr_node.split_rule\n",
    "                if test_data[i][feature] <= rule:\n",
    "                    curr_node = curr_node.left\n",
    "                else:\n",
    "                    curr_node = curr_node.right\n",
    "            predicted_labels[i] = curr_node.label\n",
    "        return predicted_labels\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, split_rule, left, right, label = None):\n",
    "        self.split_rule = split_rule\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[Split rule: \" + str(self.split_rule) + \" \" + str(self.label) + \" ]\"\n",
    "    \n",
    "def benchmark(pred_labels, true_labels):\n",
    "    errors = pred_labels != true_labels\n",
    "    err_rate = sum(errors) / float(len(true_labels))\n",
    "    indices = errors.nonzero()\n",
    "    return err_rate, indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error rate is: 0.0841067285383\n",
      "Time elapsed: 39.072932958602905\n"
     ]
    }
   ],
   "source": [
    "train, val_set, train_labels, val_labels = train_test_split(spam_train, spam_labels, test_size = int(len(spam_labels)/3))\n",
    "start = time.time()\n",
    "classifier_spam = DecisionTree(50)\n",
    "classifier_spam.train(train, train_labels)\n",
    "end = time.time()\n",
    "\n",
    "pred = classifier_spam.predict(val_set)\n",
    "err_rate, indices = benchmark(pred, val_labels)\n",
    "print(\"Validation error rate is:\", err_rate)\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, num_trees, height, m = 25):\n",
    "        self.num_trees = num_trees\n",
    "        self.height = height\n",
    "        self.m = m\n",
    "        self.trees = []\n",
    "    \n",
    "    def train(self, train_data, train_labels):\n",
    "        def sample_data(train_data, train_labels):\n",
    "            sample_list = np.random.randint(len(train_data), size = len(train_data))\n",
    "            new_train = train_data[sample_list]\n",
    "            new_labels = train_labels[sample_list]\n",
    "            new_labels.reshape((len(new_labels), 1))\n",
    "            return new_train, new_labels\n",
    "        \n",
    "        for _ in range(self.num_trees):\n",
    "            # CHANGED!\n",
    "            new_train, new_labels = sample_data(train_data, train_labels)\n",
    "            classifier = DecisionTree(self.height, random = True, m = self.m)\n",
    "            classifier.train(new_train, new_labels)\n",
    "            self.trees.append(classifier)\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        predictions = np.zeros((len(test_data), 1))\n",
    "        for tree in self.trees:\n",
    "            pred = tree.predict(test_data)\n",
    "            predictions+=pred\n",
    "        predictions/=len(self.trees)\n",
    "        final_pred = np.where(predictions > 0.5, 1, 0)\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validating for random forests (used for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error rate is: 0.0183752417795\n",
      "Time elapsed: 5736.431449174881\n"
     ]
    }
   ],
   "source": [
    "train, val_set, train_labels, val_labels = train_test_split(spam_train, spam_labels, test_size = int(len(spam_labels)/5))\n",
    "forest_spam = RandomForest(1000, 100, m=40)\n",
    "start = time.time()\n",
    "forest_spam.train(train, train_labels)\n",
    "end = time.time()\n",
    "pred = forest_spam.predict(val_set)\n",
    "err_rate, indices = benchmark(pred, val_labels)\n",
    "print(\"Validation error rate is:\", err_rate)\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = forest_spam.predict(spam_test)\n",
    "pred = pred.reshape((len(pred),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_csv(pred, \"spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "with open('census_data/train_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    samples_temp = []\n",
    "    count = False\n",
    "    for row in reader:\n",
    "        if not count:\n",
    "            first_row = row\n",
    "        if count:\n",
    "            samples_temp.append(row)\n",
    "        count = True\n",
    "        \n",
    "with open('census_data/train_data.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    samples = []\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        for key, val in row.items():\n",
    "            if key not in categorical:\n",
    "                row[key] = int(val)\n",
    "        samples.append(row)\n",
    "\n",
    "samples_temp = np.array(samples_temp)\n",
    "modes = {}\n",
    "\n",
    "def get_mode(values):\n",
    "    counts = {}\n",
    "    for i in values:\n",
    "        if i in counts:\n",
    "            counts[i]+=1\n",
    "        else:\n",
    "            counts[i]=1\n",
    "    max_freq = 0\n",
    "    mode = None\n",
    "    for key, val in counts.items():\n",
    "        if val > max_freq:\n",
    "            max_freq = val\n",
    "            mode = key\n",
    "    return mode\n",
    "\n",
    "i = 0\n",
    "for feature in samples_temp.T:\n",
    "    modes[first_row[i]] = get_mode(feature)\n",
    "    i+=1\n",
    "\n",
    "for row in samples:\n",
    "    for key, val in row.items():\n",
    "        if val == '?':\n",
    "            row[key] = modes[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting data\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(samples)\n",
    "label_index = v.feature_names_.index(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking out the label column\n",
    "census_labels = X.T[label_index].reshape(len(X), 1)\n",
    "census_data = np.delete(X.T, label_index, 0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('census_data/test_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    samples_temp = []\n",
    "    count = False\n",
    "    for row in reader:\n",
    "        if not count:\n",
    "            first_row = row\n",
    "        if count:\n",
    "            samples_temp.append(row)\n",
    "        count = True\n",
    "\n",
    "with open('census_data/test_data.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    samples = []\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        for key, val in row.items():\n",
    "            if key not in categorical:\n",
    "                row[key] = int(val)\n",
    "        samples.append(row)\n",
    "\n",
    "samples_temp = np.array(samples_temp)\n",
    "modes = {}\n",
    "\n",
    "def get_mode(values):\n",
    "    counts = {}\n",
    "    for i in values:\n",
    "        if i in counts:\n",
    "            counts[i]+=1\n",
    "        else:\n",
    "            counts[i]=1\n",
    "    max_freq = 0\n",
    "    mode = None\n",
    "    for key, val in counts.items():\n",
    "        if val > max_freq:\n",
    "            max_freq = val\n",
    "            mode = key\n",
    "    return mode\n",
    "\n",
    "i = 0\n",
    "for feature in samples_temp.T:\n",
    "    modes[first_row[i]] = get_mode(feature)\n",
    "    i+=1\n",
    "\n",
    "for row in samples:\n",
    "    for key, val in row.items():\n",
    "        if val == '?':\n",
    "            row[key] = modes[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = v.transform(samples)\n",
    "label_index = v.feature_names_.index(\"label\")\n",
    "census_test = np.delete(X.T, 22, 0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validating decision tree for census "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error rate is: 0.184726806014\n",
      "Time elapsed: 102.51969599723816\n"
     ]
    }
   ],
   "source": [
    "train, val_set, train_labels, val_labels = train_test_split(census_data, census_labels, test_size = int(len(census_labels)/3))\n",
    "classifier_census = DecisionTree(50, m = 11)\n",
    "start = time.time()\n",
    "classifier_census.train(train, train_labels)\n",
    "end = time.time()\n",
    "pred = classifier_census.predict(val_set)\n",
    "err_rate, indices = benchmark(pred, val_labels)\n",
    "print(\"Validation error rate is:\", err_rate)\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validating forests for census (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, val_set, train_labels, val_labels = train_test_split(census_data, census_labels, test_size = int(len(census_labels)/3))\n",
    "forest_census = RandomForest(500, 200, m = 11)\n",
    "start = time.time()\n",
    "forest_census.train(train, train_labels)\n",
    "end = time.time()\n",
    "pred = forest_census.predict(val_set)\n",
    "err_rate, indices = benchmark(pred, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error rate is: 0.140905757242\n",
      "Time elapsed: 5301.995204210281\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation error rate is:\", err_rate)\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = forest_census.predict(census_test)\n",
    "pred = pred.reshape((len(pred),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv(pred, \"census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 a): Extra features for spam dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the spam dataset, I parsed the spam and ham training data using the counts script that was provided in previous homeworks to see the most frequent words for both ham and spam. Then I used the bag-of-words model and used the top 400 words for each of ham and spam as my features. I also manually added some custom features such as the counts of various punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 b): Report of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree had a 8.41% error rate on the validation set. The random forest had a 2.44% error rate on the validation set. These error rates are shown in Part (1) of this notebook. My best Kaggle score for the spam dataset was 0.94283, which was obtained using random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 c): Splits in decision tree for a chosen data point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of features (copy and pasted from featurize.py output)\n",
    "# Should make this more easy to produce later, so I can generalize\n",
    "spam_feature_list = [';', '$', '#', '!', '(', '[', '&', '-', '.', '%', '\"', '1', '2', 'the', 'to', 'and', 'of',\n",
    " '3', 'a', 'in', 'you', 'for', 'this', 'is', '4', 'your', 'subject', 'with','that', '5', 's', 'be', 'or', 'on',\n",
    " '_', 'as', 'are', 'i', 'we', 'it', 'not', 'our', 'com', 'http', 'from', '6', '7', 'have', 'all', 'no', 'at', 'company',\n",
    " '0', 'will', 'by', '8', 'e', 'can', 'an', 'more', 'here', 'www', '00', 'any', 'if', '10', '9', 'information', 'font',\n",
    " 'me', 't', 'only', 'td', 'has', 'get', 'please', 'd', 'statements', 'email', 'us', 'price', 'new', 'may', 'nbsp', 'my',\n",
    " 'one', 'p', '15', 'do', 'was', 'now', 'up', 'height', 'time', 'its', 'out', 'these', '99', 'o', 'free', 'within',\n",
    " '11', 'pills', 'size', 'width', 'but', 'r', 'about', 'over', '2004', 'stock', 'b', 'other', 'message', 'money',\n",
    " '12', 'which', 'investment', 'm', 'u', 'c', 'report', 'their', '20', 'been', 'inc', 'just', 'securities', '14',\n",
    " 'business', 'online', '100', 'click', 'best', 'looking', '60', 'there', 'what', '000', 'mail', 'like', 'contact',\n",
    " 'computron', 'so', 'prices', 'align', 'future', 're', 'tr', 'x', 'news', 'need', 'products', 'v', 'into', 'net',\n",
    " '50', 'save', 'want', 'go', 'also', 'n', 'forward', '13', 'many', 'use', '25', 'microsoft', 'windows', '30', '69',\n",
    " 'today', 'would', 'color', 'they', 'million', 'international', 'th', 'link', 'software', 'face', '16', 'border',\n",
    " 'most', 'internet', 'than', 'without', 'market', 'make', 'account', 'available', 'href', 'line', 'could', 'g',\n",
    " 'professional', 'high', 'info', 'l', 'should', 'such', 'companies', 'see', 'order', 'viagra', 'world', 'office',\n",
    " 'how', '95', 'through', 'br', '90', '17', 'some', 'before', 'gas', 'act', 'number', '18', 'when', 'xp', '24',\n",
    " 'reply', 'product', '63', 'stocks', 'who', 'take', 'adobe', 'address', 'special', 'offer', 'src', 'service',\n",
    " 'site', 'don', 'advice', 'first', 'back', 'he', 'stop', 'nd', 'results', 'remove', 'pt', 'his', 'low',\n",
    " 'based', 'f', 'section', 'made', 'system', 'cialis', '21', '80', 'know', 'oil', 'security', 'home',\n",
    " 'visit', 'them', 'shares', '40', 'very', 'center', 'send', 'energy', 'after', 'soft', 'performance', '120',\n",
    " '2003', '161', 'sent', 'own', 'list', 'services', 'dollars', 'prescription', 'h', 'year', 'buy', 'day', 'off',\n",
    " 'meds', 'below', 'newsletter', 'phone', 'html', 'next', 'am', 'y', 'were', 'family', 'two', 'even', 'had',\n",
    " 'being', 'long', 'style', 'days', 'sales', 'said', 'hours', 'per', 'works', '19', 'people', '45', 'good',\n",
    " 'full', 'well', '44', 'name', 'quality', 'top', 'cd', 'paliourg', 'receive', 'does', 'she', 'because',\n",
    " 'investing', 'due', 'fact', 'every', 'find', 'details', '22', 'limited', 'offers', 'voip', 'interest',\n",
    " 'above', 'de', 'ms', '27', 'w', '32', 'pro', 'same', 'great', '23', 'via', 'life', 'then', 'events', 've',\n",
    " 'oo', 'technology', 'last', 'index', 'php', 'health', 'under', 'fax', 'much', 'bgcolor', 'sale', 'real',\n",
    " 'mg', 'look', 'part', 'china', 'read', 'hi', 'those', 'drugs', 'k', 'right', 'group', 'shipping', 'including',\n",
    " 'include', 'trade', 'less', 'industry', 'strong', 'her', 'registered', 'check', 'way', 'help', 'easy', 'further',\n",
    " '2005', 'month', 'pain', 'past', 'provided', 'pay', 'week', 'copy', 'customers', 'work', 'process', '36', 'gif', \n",
    " 'risks', 'content', 'thank', 'where', 'watch', 'mobile', 'biz', 'must', 'believe', 'web', 'cs', 'wish',\n",
    " 'expectations', 'ect', 'hou', 'enron', '2000', 'deal', 'meter', 'cc', 'pm', 'hpl', '2001', 'daren', 'thanks',\n",
    " '01', 'corp', 'mmbtu', 'j', 'forwarded', '03', 'farmer', 'let', 'attached', 'xls', '02', '04', 'contract',\n",
    " 'volume', 'robert', 'sitara', '05', 'nom', '09', '08', 'texas', 'volumes', 'questions', 'pec', 'deals', 'ena',\n",
    " 'bob', 'flow', 'file', 'change', 'production', '06', 'call', 'following', '07', '31', 'nomination', 'gary',\n",
    " 'ticket', '713', 'daily', 'mary', 'march', 'april', 'july', 'original', 'plant', 'melissa', '28', 'houston',\n",
    " 'effective', 'teco', 'vance', 'tenaska', 'fw', 'george', 'june', 'changes', '26', 'ami', 'pat', 'purchase',\n",
    " 'julie', 'smith', 'set', '29', 'transport', 'jackie', 'agreement', 'north', 'aimee', 'january', 'noms', 'lisa',\n",
    " 'tap', 'desk', 'america', 'susan', 'david', 'pipeline', 'october', 'actuals', 'hsc', 'michael', 'mark', 'each',\n",
    " 'cotten', 'needs', 'think', 'august', 'management', 'taylor', 'megan', 'iv', 'steve', 'chokshi', 'friday',\n",
    " 'request', 'delivery', 'john', 'fyi', 'tom', 'duke', 'hplc', 'point', 'co', 'date', 'september', 'did', 'tu',\n",
    " 'november', 'wellhead', 'december', 'clynes', 'february', 'eastrans', 'total', 'feb', 'activity', 'spot', 'lloyd',\n",
    " 'tickets', 'issue', 'counterparty', 'meeting', 'sure', 'graves', 'unify', 'currently', 'fuel', 'brian', 'team',\n",
    " 'monday', 'll', 'meters', 'aol', 'resources', 'revised', 'na', 'until', 'lee', '98', 'still', 'actual', 'down',\n",
    " 'utilities', 'pg', 'supply', 'global', 'contracts', 'txu', 'jan', 'tx', 'meyers', 'both', 'howard', 'hplno',\n",
    " 'rate', 'rita', 'thursday', 'fee', 'give', 'lannou', 'received', 'again', 'created', 'end', 'nominations',\n",
    " 'hanks', 'between', 'since', 'young', 'james', 'going', 'note', 'allocation', 'trading', 'demand', 'entered']\n",
    "\n",
    "spam_feature_mapping = {}\n",
    "for i in range(len(spam_feature_list)):\n",
    "    spam_feature_mapping[i] = spam_feature_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_splits(tree, data, mapping):\n",
    "    node = tree.root\n",
    "    while node.label == None:\n",
    "        feature, thresh = node.split_rule\n",
    "        data_val = data[feature]\n",
    "        if data_val <= thresh:\n",
    "            print(mapping[feature], \"<=\", thresh)\n",
    "            node = node.left\n",
    "        else:\n",
    "            print(mapping[feature], \">\", thresh)\n",
    "            node = node.right\n",
    "    print(\"Data point classified as\", node.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron <= 0.0\n",
      "! <= 0.0\n",
      "hpl <= 0.0\n",
      "thanks > 0.0\n",
      "http <= 1.0\n",
      ". > 0.0\n",
      "and <= 12.0\n",
      "Data point classified as 0\n"
     ]
    }
   ],
   "source": [
    "list_splits(classifier_spam, spam_test[2], spam_feature_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 d): Most common splits at root for random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def common_splits(forest, mapping):\n",
    "    split_rules_counts = {}\n",
    "    for tree in forest.trees:\n",
    "        root = tree.root\n",
    "        split_rule = root.split_rule\n",
    "        if split_rule not in split_rules_counts:\n",
    "            split_rules_counts[split_rule] = 1\n",
    "        else:\n",
    "            split_rules_counts[split_rule] += 1\n",
    "    print(\"Most common splits:\")\n",
    "    for _ in range(5):\n",
    "        best = max(split_rules_counts, key=lambda i: split_rules_counts[i])\n",
    "        print(mapping[best[0]], \"<=\", best[1], \"(\" + str(split_rules_counts[best]) + \" occurences)\")\n",
    "        del split_rules_counts[best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common splits:\n",
      "sitara <= 0.0 (11 occurences)\n",
      "http <= 0.0 (10 occurences)\n",
      "questions <= 0.0 (9 occurences)\n",
      "enron <= 0.0 (9 occurences)\n",
      "pm <= 0.0 (9 occurences)\n"
     ]
    }
   ],
   "source": [
    "common_splits(forest_spam, spam_feature_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 a): Extra features for census dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variables, I followed the spec's \"Easy\" suggestion, and just mapped each category to a binary variable, effectively creating new feature columns for each value in a categorical variable. As for dealing with missing attributes, I employed the \"Easy\" technique suggested by the spec and simply replaced the missing value with the mode of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 b): Report of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree had a 18.47% error rate on the validation set. The random forest had a 14.00% error rate on the validation set. These error rates are shown in Part (1) of this notebook. My best Kaggle score for the census dataset was 0.78427, which was obtained using random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 c): Splits in decision tree for a chosen data point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_feature_mapping = {}\n",
    "v.feature_names_\n",
    "count = 0\n",
    "for item in v.feature_names_:\n",
    "    if item != 'label':\n",
    "        census_feature_mapping[count] = item\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marital-status=Married-AF-spouse <= 0.0\n",
      "capital-gain <= 6849.0\n",
      "education-num > 12.0\n",
      "age <= 32.0\n",
      "hours-per-week <= 44.0\n",
      "capital-loss <= 2001.0\n",
      "marital-status=Divorced <= 0.0\n",
      "capital-loss <= 1504.0\n",
      "capital-gain > 4101.0\n",
      "age > 29.0\n",
      "Data point classified as 0\n"
     ]
    }
   ],
   "source": [
    "list_splits(classifier_census, census_test[2], census_feature_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 d): Most common splits at root for random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common splits:\n",
      "relationship=Other-relative <= 0.0 (12 occurences)\n",
      "marital-status=Married-AF-spouse <= 0.0 (9 occurences)\n",
      "education-num <= 12.0 (8 occurences)\n",
      "race=White <= 0.0 (8 occurences)\n",
      "marital-status=Married-spouse-absent <= 0.0 (7 occurences)\n"
     ]
    }
   ],
   "source": [
    "common_splits(forest_census, census_feature_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stopping criteria, I have a depth parameter which makes it so that a node in the decision tree stops growing once it reaches a certain depth. The only splitting criteria I used was weighted entropy. As for dealing with missing attributes, I employed the \"Easy\" technique suggested by the spec and simply replaced the missing value with the mode of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random forests, I implement bagging for both the samples and the features. I do the techniques suggested in lecture, where for each tree, I create the data matrices by randomly sampling with replacement and creating a size n x d matrix, where n is the number of original samples. For features, I only split on sqrt(d) features where d is the number of features. The decision tree I used for the forests only differed in the bagging techniques used, but were otherwise the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy of featurize.py code and count.py code (For reference, not to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    **************** PLEASE READ ***************\n",
    "    \n",
    "    Script that reads in spam and ham messages and converts each training example\n",
    "    into a feature vector\n",
    "    \n",
    "    Code intended for UC Berkeley course CS 189/289A: Machine Learning\n",
    "    \n",
    "    Requirements:\n",
    "    -scipy ('pip install scipy')\n",
    "    \n",
    "    To add your own features, create a function that takes in the raw text and\n",
    "    word frequency dictionary and outputs a int or float. Then add your feature\n",
    "    in the function 'def generate_feature_vector'\n",
    "    \n",
    "    The output of your file will be a .mat file. The data will be accessible using\n",
    "    the following keys:\n",
    "    -'training_data'\n",
    "    -'training_labels'\n",
    "    -'test_data'\n",
    "    \n",
    "    Please direct any bugs to kevintee@berkeley.edu\n",
    "    '''\n",
    "\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import re\n",
    "import scipy.io\n",
    "\n",
    "NUM_TRAINING_EXAMPLES = 5172\n",
    "NUM_TEST_EXAMPLES = 5857\n",
    "\n",
    "BASE_DIR = './'\n",
    "SPAM_DIR = 'spam/'\n",
    "HAM_DIR = 'ham/'\n",
    "TEST_DIR = 'test/'\n",
    "\n",
    "# ************* Features *************\n",
    "\n",
    "# Features that look for certain words\n",
    "def freq_pain_feature(text, freq):\n",
    "    return float(freq['pain'])\n",
    "\n",
    "def freq_private_feature(text, freq):\n",
    "    return float(freq['private'])\n",
    "\n",
    "def freq_bank_feature(text, freq):\n",
    "    return float(freq['bank'])\n",
    "\n",
    "def freq_money_feature(text, freq):\n",
    "    return float(freq['money'])\n",
    "\n",
    "def freq_drug_feature(text, freq):\n",
    "    return float(freq['drug'])\n",
    "\n",
    "def freq_spam_feature(text, freq):\n",
    "    return float(freq['spam'])\n",
    "\n",
    "def freq_prescription_feature(text, freq):\n",
    "    return float(freq['prescription'])\n",
    "\n",
    "def freq_creative_feature(text, freq):\n",
    "    return float(freq['creative'])\n",
    "\n",
    "def freq_height_feature(text, freq):\n",
    "    return float(freq['height'])\n",
    "\n",
    "def freq_featured_feature(text, freq):\n",
    "    return float(freq['featured'])\n",
    "\n",
    "def freq_differ_feature(text, freq):\n",
    "    return float(freq['differ'])\n",
    "\n",
    "def freq_width_feature(text, freq):\n",
    "    return float(freq['width'])\n",
    "\n",
    "def freq_other_feature(text, freq):\n",
    "    return float(freq['other'])\n",
    "\n",
    "def freq_energy_feature(text, freq):\n",
    "    return float(freq['energy'])\n",
    "\n",
    "def freq_business_feature(text, freq):\n",
    "    return float(freq['business'])\n",
    "\n",
    "def freq_message_feature(text, freq):\n",
    "    return float(freq['message'])\n",
    "\n",
    "def freq_volumes_feature(text, freq):\n",
    "    return float(freq['volumes'])\n",
    "\n",
    "def freq_revision_feature(text, freq):\n",
    "    return float(freq['revision'])\n",
    "\n",
    "def freq_path_feature(text, freq):\n",
    "    return float(freq['path'])\n",
    "\n",
    "def freq_meter_feature(text, freq):\n",
    "    return float(freq['meter'])\n",
    "\n",
    "def freq_memo_feature(text, freq):\n",
    "    return float(freq['memo'])\n",
    "\n",
    "def freq_planning_feature(text, freq):\n",
    "    return float(freq['planning'])\n",
    "\n",
    "def freq_pleased_feature(text, freq):\n",
    "    return float(freq['pleased'])\n",
    "\n",
    "def freq_record_feature(text, freq):\n",
    "    return float(freq['record'])\n",
    "\n",
    "def freq_out_feature(text, freq):\n",
    "    return float(freq['out'])\n",
    "\n",
    "# Features that look for certain characters\n",
    "def freq_semicolon_feature(text, freq):\n",
    "    return text.count(';')\n",
    "\n",
    "def freq_dollar_feature(text, freq):\n",
    "    return text.count('$')\n",
    "\n",
    "def freq_sharp_feature(text, freq):\n",
    "    return text.count('#')\n",
    "\n",
    "def freq_exclamation_feature(text, freq):\n",
    "    return text.count('!')\n",
    "\n",
    "def freq_para_feature(text, freq):\n",
    "    return text.count('(')\n",
    "\n",
    "def freq_bracket_feature(text, freq):\n",
    "    return text.count('[')\n",
    "\n",
    "def freq_and_feature(text, freq):\n",
    "    return text.count('&')\n",
    "\n",
    "# added here\n",
    "\n",
    "def freq_subscribe_feature(text, freq):\n",
    "    return float(freq['subscribe'])\n",
    "\n",
    "def freq_unsubscribe_feature(text, freq):\n",
    "    return float(freq['unsubscrime'])\n",
    "\n",
    "def freq_dash_feature(text, freq):\n",
    "    return text.count('-')\n",
    "\n",
    "def freq_dot_feature(text, freq):\n",
    "    return text.count('.')\n",
    "\n",
    "def freq_fast_feature(text, freq):\n",
    "    return text.count('fast')\n",
    "\n",
    "def freq_trial_feature(text, freq):\n",
    "    return text.count('trial')\n",
    "\n",
    "def freq_product_feature(text, freq):\n",
    "    return text.count('product')\n",
    "\n",
    "def freq_thank_feature(text, freq):\n",
    "    return text.count('thank')\n",
    "\n",
    "def freq_click_feature(text, freq):\n",
    "    return text.count('click')\n",
    "\n",
    "def freq_link_feature(text, freq):\n",
    "    return text.count('link')\n",
    "\n",
    "def freq_free_feature(text, freq):\n",
    "    return text.count('free')\n",
    "\n",
    "def freq_discount_feature(text, freq):\n",
    "    return text.count('discount')\n",
    "\n",
    "def freq_sale_feature(text, freq):\n",
    "    return text.count('sale')\n",
    "\n",
    "def freq_buy_feature(text, freq):\n",
    "    return text.count('buy')\n",
    "\n",
    "def freq_payment_feature(text, freq):\n",
    "    return text.count('payment')\n",
    "\n",
    "def freq_saving_feature(text, freq):\n",
    "    return text.count('saving')\n",
    "\n",
    "def freq_savings_feature(text, freq):\n",
    "    return text.count('savings')\n",
    "\n",
    "#more\n",
    "\n",
    "def freq_http_feature(text, freq):\n",
    "    return text.count('http')\n",
    "\n",
    "def freq_made_feature(text, freq):\n",
    "    return text.count('made')\n",
    "\n",
    "def freq_warranty_feature(text, freq):\n",
    "    return text.count('warranty')\n",
    "\n",
    "def freq_percent_feature(text, freq):\n",
    "    return text.count('%')\n",
    "\n",
    "def freq_you_feature(text, freq):\n",
    "    return text.count('you')\n",
    "\n",
    "def freq_heart_feature(text, freq):\n",
    "    return text.count('heart')\n",
    "\n",
    "def freq_sale_feature(text, freq):\n",
    "    return text.count('sale')\n",
    "\n",
    "def freq_low_feature(text, freq):\n",
    "    return text.count('low')\n",
    "\n",
    "def freq_price_feature(text, freq):\n",
    "    return text.count('price')\n",
    "\n",
    "def freq_prices_feature(text, freq):\n",
    "    return text.count('prices')\n",
    "\n",
    "def freq_www_feature(text, freq):\n",
    "    return text.count('www')\n",
    "\n",
    "def freq_quotes_feature(text, freq):\n",
    "    return text.count('\\\"')\n",
    "\n",
    "def generate_all_spam():\n",
    "    with open('spam/counts') as f:\n",
    "        content = f.readlines(1)\n",
    "    return content\n",
    "# --------- Add your own feature methods ----------\n",
    "def example_feature(text, freq):\n",
    "    return int('example' in text)\n",
    "\n",
    "# Generates a feature vector\n",
    "def generate_feature_vector(text, freq, list_of_words, list_of_words2):\n",
    "    feature = []\n",
    "    feature.append(freq_semicolon_feature(text, freq))\n",
    "    feature.append(freq_dollar_feature(text, freq))\n",
    "    feature.append(freq_sharp_feature(text, freq))\n",
    "    feature.append(freq_exclamation_feature(text, freq))\n",
    "    feature.append(freq_para_feature(text, freq))\n",
    "    feature.append(freq_bracket_feature(text, freq))\n",
    "    feature.append(freq_and_feature(text, freq))\n",
    "    feature.append(freq_dash_feature(text, freq))\n",
    "    feature.append(freq_dot_feature(text, freq))\n",
    "    feature.append(freq_percent_feature(text, freq))\n",
    "    feature.append(freq_quotes_feature(text, freq))\n",
    "    used = list(avoid)\n",
    "    for i in list_of_words[:400]:\n",
    "        if not i in used:\n",
    "            feature.append(float(freq[i]))\n",
    "            used.append(i)\n",
    "    for i in list_of_words2[:400]:\n",
    "        if not i in used:\n",
    "            feature.append(float(freq[i]))\n",
    "            used.append(i)\n",
    "    print(used)\n",
    "    return feature\n",
    "\n",
    "avoid = [';', '$', '#', '!', '(', '[', '&', '-', '.', '%', '\\\"']\n",
    "total_list = []\n",
    "\n",
    "with open('spam/counts.txt') as f:\n",
    "    for line in f:\n",
    "        total_list.append(line.split(' ')[0])\n",
    "total_list2 = []\n",
    "with open('ham/counts.txt') as f:\n",
    "    for line in f:\n",
    "        total_list2.append(line.split(' ')[0])\n",
    "\n",
    "# This method generates a design matrix with a list of filenames\n",
    "# Each file is a single training example\n",
    "def generate_design_matrix(filenames):\n",
    "    design_matrix = []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            text = f.read() # Read in text from file\n",
    "            text = text.replace('\\r\\n', ' ') # Remove newline character\n",
    "            words = re.findall(r'\\w+', text)\n",
    "            word_freq = defaultdict(int) # Frequency of all words\n",
    "            for word in words:\n",
    "                word_freq[word] += 1\n",
    "            \n",
    "            # Create a feature vector\n",
    "            feature_vector = generate_feature_vector(text, word_freq, total_list, total_list2)\n",
    "            design_matrix.append(feature_vector)\n",
    "    return design_matrix\n",
    "\n",
    "# ************** Script starts here **************\n",
    "# DO NOT MODIFY ANYTHING BELOW\n",
    "\n",
    "spam_filenames = glob.glob(BASE_DIR + SPAM_DIR + '*.txt')\n",
    "spam_design_matrix = generate_design_matrix(spam_filenames)\n",
    "ham_filenames = glob.glob(BASE_DIR + HAM_DIR + '*.txt')\n",
    "ham_design_matrix = generate_design_matrix(ham_filenames)\n",
    "# Important: the test_filenames must be in numerical order as that is the\n",
    "# order we will be evaluating your classifier\n",
    "test_filenames = [BASE_DIR + TEST_DIR + str(x) + '.txt' for x in range(NUM_TEST_EXAMPLES)]\n",
    "test_design_matrix = generate_design_matrix(test_filenames)\n",
    "\n",
    "X = spam_design_matrix + ham_design_matrix\n",
    "Y = [1]*len(spam_design_matrix) + [0]*len(ham_design_matrix)\n",
    "\n",
    "file_dict = {}\n",
    "file_dict['training_data'] = X\n",
    "file_dict['training_labels'] = Y\n",
    "file_dict['test_data'] = test_design_matrix\n",
    "scipy.io.savemat('spam_data.mat', file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import re\n",
    "\n",
    "WORD_COUNTS = {}\n",
    "def count(text, freq):\n",
    "    for word, count in freq.items():\n",
    "        word = word.lower()\n",
    "        if word not in WORD_COUNTS:\n",
    "            WORD_COUNTS[word] = 0\n",
    "        WORD_COUNTS[word] += count\n",
    "\n",
    "filenames = glob.glob('*.txt')\n",
    "for filename in filenames:\n",
    "    with open(filename) as f:\n",
    "        text = f.read() # Read in text from file\n",
    "        text = text.replace('\\r\\n', ' ') # Remove newline character\n",
    "        words = re.findall(r'\\w+', text)\n",
    "        freq = defaultdict(int) # Frequency of all words\n",
    "        for word in words:\n",
    "            freq[word] += 1\n",
    "        count(text, freq)\n",
    "\n",
    "WORDS = sorted(WORD_COUNTS, reverse=True, key=lambda x: WORD_COUNTS[x])\n",
    "\n",
    "with open('counts.txt', 'w') as f:\n",
    "    for word in WORDS:\n",
    "        f.write('{} {}\\n'.format(word, WORD_COUNTS[word]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
